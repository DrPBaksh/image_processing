{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5212f3a7-18a3-40f9-bfd1-7b75496c0ffb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create our very own image classification algorithm\n",
    "\n",
    "Today we are going to have a look at how you can perform your own end to end image classification project. In many of your roles you may not think you have use cases for image classification, and that may be true however these are some examples of where it could be applied in an industry such as BP\n",
    "- Satilitate images - oil spill detection\n",
    "- Health and safety detection -  Is the person wearing a helmet or goggles?\n",
    "- Enginnering component checks -  Automatically scan through pictures of equipment searching for cracks\n",
    "- Satilite images for land use classification - If you are building a new pipeline based on your planned route can you estimate the cost based on the current land use?\n",
    "\n",
    "\n",
    "I should note we are doing ***image classification** today. That is take an image and from the the pixel values in that image apply it to a category or class. There are more advanced image classification algroithms that look to segment every pixel in the image and applly that to a category\n",
    "\n",
    "![classifciation](https://miro.medium.com/v2/resize:fit:1400/1*Hz6t-tokG1niaUfmcysusw.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd28f67-c697-4dff-a74b-c57c9396aa09",
   "metadata": {},
   "source": [
    "## Stages\n",
    "1. Problem Statement\n",
    "2. Collect images\n",
    "3. Inspect images\n",
    "4. Clean images\n",
    "5. Use a CNN network to perform custom image classification\n",
    "6. Evaluate results\n",
    "7. Test on a new image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf9a536-f283-4be1-a324-9da571a34d10",
   "metadata": {},
   "source": [
    "### What are you classifying? \n",
    "\n",
    "In this workshop we are going to attempt to make an algorithm that will classify cars from lorrys from vans all from scratch. This could be a use case for BP. Knowing the proportion of vehicles at a locaiton could inform business strategy for example. \n",
    "\n",
    "I will also share with you a script that allows you to create a classifier on a topic you are more interested in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c028d5-a5c6-4ad3-9893-b1ecb119b863",
   "metadata": {},
   "source": [
    "## Generating your own Dataset\n",
    "This section will be different to what we saw in the workshop since we took an already prepared dataset. In order to generate your dataset you will:\n",
    "- Use a bing images api to collect images that belong to a certain search category\n",
    "- Validate these images - check they are sensible\n",
    "- Tranform these images- Force them to be the same dimensions\n",
    "- Augment** these images to increase the size of our dataset\n",
    "\n",
    "First lets install our packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c35971-bba8-40ab-bedf-5e00fb3f64f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python\n",
    "# !pip install imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c69c6eb3-5271-4890-abc0-8ace8fac4403",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf \n",
    "\n",
    "print(keras.__version__)\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43deddde-803c-4793-8698-da37e471600e",
   "metadata": {},
   "source": [
    "### Getting images from bing \n",
    "Instructions\n",
    "1. Install bing image downloader using pip\n",
    "2. Run the cell below with your categories. This may take some time. You should see folders inside dataset populate with your data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d725f1-9575-4f93-a80e-5da5e1805795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bing_image_downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd0df78-6cb0-4585-a7fa-af8a67fc3f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bing_image_downloader import downloader\n",
    "search_categories = ['....', '....'] # e.g ['cats', 'dogs']\n",
    "# search_categories = ['cats', 'dogs']\n",
    "for search_string in search_categories:\n",
    "    downloader.download(search_string, limit=100,  output_dir='dataset', force_replace=False, timeout=60, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a06dd7-2526-4756-acc7-62e8eefbea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder path containing images\n",
    "print('When Image opons press \"y\" to keep image, \"q\" to exit the process or any other key e.g \"n\" to remove the image from our images')\n",
    "\n",
    "images_to_keep = {}\n",
    "for category in search_categories:\n",
    "    print('clasifing ', category)\n",
    "    folder_path = os.path.join('dataset', category)\n",
    "    \n",
    "    image_files = os.listdir(folder_path)\n",
    "    keystrokes = []\n",
    "    for image_file in tqdm(image_files):\n",
    "        # Open image file\n",
    "        try:\n",
    "            image_path = os.path.join(folder_path, image_file)\n",
    "            image = cv2.imread(image_path)\n",
    "            cv2.imshow('Image', image)\n",
    "            key = cv2.waitKey(0)\n",
    "            keystrokes.append(key)\n",
    "            if key == ord('q'):\n",
    "                breakq\n",
    "        except:\n",
    "            print('error in image load')\n",
    "            keystrokes.append(200)\n",
    "    cv2.destroyAllWindows()\n",
    "    images_to_keep[category] = keystrokes\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99472425-d5f3-46fd-8b96-8a4d910f3e86",
   "metadata": {},
   "source": [
    "### Lets view the images we removed\n",
    "\n",
    "Below we can just look over the images we manually removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79d7b65-c518-4917-a254-2a8a290c5a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## if you skipped the last step\n",
    "# with open(\"images_to_keep.json\") as f:\n",
    "#     data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d7d70a-b534-47f1-9c79-770e42ca0a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "### clean our seleted images\n",
    "\n",
    "counter = 0\n",
    "for category in search_categories:\n",
    "    keystrokes = images_to_keep[category]\n",
    "    folder_path = os.path.join('dataset', category)\n",
    "    image_files = os.listdir(folder_path)\n",
    "    im_index = 0\n",
    "    for keystroke in keystrokes:\n",
    "        if keystroke != 121:\n",
    "            try:\n",
    "                image_path = os.path.join(folder_path, image_files[im_index])\n",
    "                image = cv2.imread(image_path)\n",
    "                plt.subplot(5,5,counter +1)\n",
    "                plt.imshow(image)\n",
    "                plt.title('Removed from : ' + category)\n",
    "                plt.axis('tight')\n",
    "                plt.xticks([])\n",
    "                plt.yticks([])\n",
    "                counter +=1\n",
    "                if counter == 25:\n",
    "                    fig = plt.gcf()\n",
    "                    fig.set_size_inches(15, 15)\n",
    "                    plt.show()\n",
    "                    counter = 0\n",
    "            except:\n",
    "                pass\n",
    "        im_index+= 1\n",
    "            \n",
    "            \n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 15)\n",
    "plt.show()            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22ad6d1-b5c7-42be-b549-bdd31b9c61ae",
   "metadata": {},
   "source": [
    "## Image Augmentation\n",
    "\n",
    "Because we do not have a huge number of images in each class we are going to increase the number in each category using image augmentaiton\n",
    "Image augmentaiton:\n",
    "\n",
    "*Image augmentation refers to a set of techniques used to artificially increase the size of a dataset of images for machine learning or computer vision tasks. These techniques can be used to improve the performance of machine learning models by introducing variations of the original images, which can help the models to generalize better and reduce overfitting.*\n",
    "\n",
    "*There are many types of image augmentation techniques, including:*\n",
    "\n",
    "*Flipping or mirroring: the image is flipped horizontally or vertically*\n",
    "\n",
    "*Rotation: the image is rotated by a certain angle*\n",
    "\n",
    "*Scaling: the image is scaled up or down*\n",
    "\n",
    "*Translation: the image is shifted horizontally or vertically*\n",
    "\n",
    "*Cropping: a portion of the image is cropped out*\n",
    "\n",
    "*Adding noise: random noise is added to the image*\n",
    "\n",
    "*Changing brightness or contrast: the brightness or contrast of the image is modified*\n",
    "\n",
    "*Changing color: the color of the image is modified*\n",
    "\n",
    "By applying these techniques to the original images, we can generate many new images with variations that are still similar to the original images. These augmented images can be used to train machine learning models, which can then better recognize and classify new images in the future.\n",
    "\n",
    "We can look at a quick example below\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91378270-a05a-4e48-9b75-67a7f0039854",
   "metadata": {},
   "outputs": [],
   "source": [
    " ## Get the original image\n",
    "url = \"https://www.hepper.com/wp-content/uploads/2021/11/shutterstock_164646389.jpg\"\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bd75f9-fbd4-49a8-9fd3-0d72f814ad23",
   "metadata": {},
   "source": [
    "Make a dictionary of the augmentations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c0cb52-627d-43b2-a26d-e163b7119382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug.augmenters as iaa\n",
    "aug_seq = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5),        # flip horizontally with probability 0.5\n",
    "    iaa.Affine(rotate=(-100, 100), scale=(0.8, 1.2)),   # rotate and scale\n",
    "    iaa.GaussianBlur(sigma=(0, 0.2)),    # apply gaussian blur with sigma between 0 and 3.0\n",
    "    iaa.AdditiveGaussianNoise(scale=(0, 0.05*255))    # add gaussian noise\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d142ee-1a15-4743-9ba9-9c1c2ef9595a",
   "metadata": {},
   "source": [
    "View the effect of the augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833cb63c-88b5-4c22-bc65-c4b1ca9fe070",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    aug_img = aug_seq.augment_image(np.array(img))\n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    plt.imshow(aug_img)\n",
    "    plt.axis('tight')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9434143-cb26-41db-b684-d2d38e61a8dc",
   "metadata": {},
   "source": [
    "**Apply Augmentations on our data**\n",
    "\n",
    "We shall now apply the augmentations to all of the images that we selected from our bing search\n",
    "\n",
    "**Note**\n",
    "We are going to use approximately the first 20 images for testing. We will make sure we do not apply augmentations to those. The rest of the images will be used for training and we can augment these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee37d67e-c9d7-409d-a04e-10aa1197fca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list_train = []\n",
    "target_list_train = []\n",
    "\n",
    "image_list_test = []\n",
    "target_list_test = []\n",
    "\n",
    "for category in search_categories:\n",
    "    keystrokes = images_to_keep[category]\n",
    "    folder_path = os.path.join('dataset', category)\n",
    "    image_files = os.listdir(folder_path)\n",
    "    im_index = 0\n",
    "    for keystroke in keystrokes:\n",
    "        if keystroke == 121:\n",
    "            if im_index < 20:\n",
    "                image_path = os.path.join(folder_path, image_files[im_index])\n",
    "                image = cv2.imread(image_path)\n",
    "                resized_img = cv2.resize(image, (128, 128))\n",
    "                image_list_test.append(resized_img)\n",
    "                target_list_test.append(category)\n",
    "            else:\n",
    "                for i in range(10):\n",
    "                    image_path = os.path.join(folder_path, image_files[im_index])\n",
    "                    image = cv2.imread(image_path)\n",
    "                    resized_img = cv2.resize(image, (128, 128))\n",
    "                    aug_img = aug_seq.augment_image(resized_img)\n",
    "                    image_list_train.append(aug_img)\n",
    "                    target_list_train.append(category)\n",
    "        im_index+= 1\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772906d3-340d-450c-a75b-c9a98893f07e",
   "metadata": {},
   "source": [
    "Define X_train, X_test. y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c36d549-159f-4240-b0f1-b9066a4fe4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(image_list_train)\n",
    "y_train = np.array(target_list_train)\n",
    "X_test = np.array(image_list_test)\n",
    "y_test = np.array(target_list_test)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbab9488-1e45-4ca0-b2a5-7d741f856222",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images = 40\n",
    "subplot_x, subplot_y = int(sample_images**0.5), int(sample_images**0.5)\n",
    "image_num = 1\n",
    "for i in range(int(subplot_x * subplot_y)):\n",
    "    plt.subplot(subplot_x, subplot_y, image_num)\n",
    "    num = random.randint(1, len(X_train))\n",
    "    plt.imshow(X_train[num, :, :, :])\n",
    "    plt.title(y_train[num])\n",
    "    plt.axis('tight')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    image_num += 1\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6999a41b-e60d-4909-9a9a-805fcf4e48c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class2label = dict(zip(search_categories, range(len(search_categories))))\n",
    "label2class = dict(zip( range(len(search_categories)), search_categories))\n",
    "y_train_encoded = pd.DataFrame(y_train).iloc[:, 0].map(class2label).values\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_train_onehot = onehot_encoder.fit_transform(y_train_encoded.reshape(-1, 1))\n",
    "\n",
    "y_test_encoded = pd.DataFrame(y_test).iloc[:, 0].map(class2label).values\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_test_onehot = onehot_encoder.fit_transform(y_test_encoded.reshape(-1, 1))\n",
    "\n",
    "## Manual check that the encoding looks correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac6d1be-b592-4fee-bf26-78f9f4223be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape, y_test_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb49bda-c051-42f3-940c-5ce7bbac5886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class AccuracyHistory(Callback):\n",
    "    def __init__(self, plot_interval=3):\n",
    "        super(AccuracyHistory, self).__init__()\n",
    "        self.plot_interval = plot_interval\n",
    "        self.accuracy = []\n",
    "        self.val_accuracy = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.accuracy.append(logs.get('accuracy'))\n",
    "        self.val_accuracy.append(logs.get('val_accuracy'))\n",
    "        if epoch % self.plot_interval == 0:\n",
    "            plt.plot(self.accuracy, label='Training Accuracy')\n",
    "            plt.plot(self.val_accuracy, label='Validation Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0091b1-6e8f-4e47-94d4-0ec71776b329",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network with Transfer Learning!\n",
    "\n",
    "This is advanced stuff we are using. I will do my best to explain as simply as possible\n",
    "\n",
    "- Convolutional Neural Networks are models that attempt to learn the best spatial filters in order to design a network to meet their classificaiton task\n",
    "\n",
    "- Imagine you have a childs toy with various shapes. You have an idea of the types of shapes you may get and you need to make physical filters in order to identify shapes in to circles stars arrows and rectangles. Using your prior knowledge you may start of by designing a first layer of filters as shown below. These filters separate out the stars and the cicles from the square rectangle and arrow. You then have to make a further layer of filters to filter the rest down to each object. You are using your past knowledge of standard shapes to design these filters to the problem. In a CNN these layers / filters are learnt from the data \n",
    "\n",
    "<img src = \"https://raw.githubusercontent.com/DrPBaksh/workshop-data/main/cnn_diagram_1.PNG\"  width=\"500\" height=\"400\">\n",
    "\n",
    "CNN learn these best filters.\n",
    "\n",
    "The standard type of digagram used to show a CNN arhitecutre is below\n",
    "\n",
    "<img src = 'https://production-media.paperswithcode.com/method_collections/cnn.jpeg' width=\"500\" height=\"400\">\n",
    "\n",
    "The image at the begining is input to the network. The shape of that input is the shape in pixels and colour dimensions of the image. It then goes through lots of complicated convilutional layerss before being connected up to an output that is the size of the number of categories that we are using\n",
    "\n",
    "\n",
    "\n",
    "**Transfer Learning**\n",
    "\n",
    "There are only a couple of points I want you to think about with transfer learning\n",
    "- CNNs netorks work best when their weights (the strength of their connections) and the design of their filters has been trained on very large datasets \n",
    "- In image classification you can often use a network that has been pre trained on a huge amount of images to learn filters and connections and then you tweak the end of the network for your purpose\n",
    "- Image teaching a baby how to classify dogs and cats. First the baby needs to learn how to understand images. It needs to learn that different things are different colours. It needs to learn that the same object can appear in different sizes etc. Once that baby has learnt more about images in general you can then get it to learn the difference between a cat and a dog through examples. In transfer learning we do not start from scratch. We start with a human who has already experience the world through their eyes. We now give them some pictures of dogs and cats and they are better placed to begin identifying them \n",
    "\n",
    "\n",
    "<img src = 'https://www.researchgate.net/profile/Mazin-Mohammed/publication/347946990/figure/fig3/AS:973645212049408@1609146516624/Example-of-transfer-learning-in-medical-images.png\n",
    "' width=\"500\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b642cf3-2205-41c6-a7a6-54b4e0d667d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Our neural network\n",
    "# We take a pre trained model but say can we change the top layer for our image shape\n",
    "base_model = tf.keras.applications.EfficientNetB0(include_top=False) \n",
    "# We say we do not want to retrain anything in this network at the moment\n",
    "base_model.trainable = False\n",
    "# We tell the model that we are going to inpouting images of shape 128, 128 into the network\n",
    "inputs = tf.keras.layers.Input(shape=(128, 128, 3), name=\"input_layer\")\n",
    "# Apply that inoput layer on the top of our pre trained model\n",
    "x = base_model(inputs)\n",
    "# We are going to add a layer onto the imported network that it can learn the specifics for the cat and dog\n",
    "x = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
    "# We are going to have an iput layer equal to the number of categories\n",
    "outputs = tf.keras.layers.Dense(len(search_categories), activation=\"softmax\", name=\"output_layer\")(x)\n",
    "# COmpile that model together\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "# Apply the settings for the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# fit the model to the data\n",
    "history = model.fit(X_train, y_train_onehot, epochs=10, validation_data=(X_test, y_test_onehot), callbacks=[AccuracyHistory(plot_interval=3)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e387789f-78ef-4dde-93e5-6081aa8fca6b",
   "metadata": {},
   "source": [
    "## Evaluate the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d7427a-c13c-41d1-b650-16bd03bcda50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_onehot)\n",
    "print(\"Test loss:\", test_loss)\n",
    "print(\"Test accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f154d2bb-24fe-45d1-be4c-11049976768c",
   "metadata": {},
   "source": [
    "Below we plot the results from our testing data and view the networks classificaiton on our images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562bd6c7-0241-48be-9a7f-a92f63f3be9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images = 40\n",
    "subplot_x, subplot_y = int(sample_images**0.5), int(sample_images**0.5)\n",
    "image_num = 1\n",
    "for i in range(int(subplot_x * subplot_y)):\n",
    "    plt.subplot(subplot_x, subplot_y, image_num)\n",
    "    num = random.randint(1, len(X_test))\n",
    "    test_image = np.expand_dims(X_test[num-1, :, :, :], axis = 0)\n",
    "    plt.imshow(X_test[num-1, :, :, :])\n",
    "    prediction = model.predict(test_image)\n",
    "    class_prediction = np.argmax(prediction, axis=1)\n",
    "    plt.title('truth: '+ y_test[num -1] + ' prediction: ' + label2class[class_prediction[0]] )\n",
    "    plt.axis('tight')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    image_num += 1\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(20, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9836ea22-b568-44e8-a95e-2caed33296da",
   "metadata": {},
   "source": [
    "Lets look at the classificaiton report on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7f046c-43fd-41e7-927f-5e3964dacc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_predictions, axis=1)\n",
    "y_pred = [label2class[i] for i in y_pred_classes]\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6dcb28-4094-463a-a3ac-a63039e248b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_names = search_categories\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "ax.set(xticks=np.arange(cm.shape[1]),\n",
    "       yticks=np.arange(cm.shape[0]),\n",
    "       xticklabels=class_names,\n",
    "       yticklabels=class_names,\n",
    "       title='Confusion matrix',\n",
    "       ylabel='True label',\n",
    "       xlabel='Predicted label')\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2168cc9-9377-4290-abca-60f3500451f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(int(subplot_x * subplot_y)):\n",
    "    plt.subplot(subplot_x, subplot_y, image_num)\n",
    "    num = random.randint(1, len(X_test))\n",
    "    test_image = np.expand_dims(X_test[num-1, :, :, :], axis = 0)\n",
    "    plt.imshow(X_test[num-1, :, :, :])\n",
    "    plt.title('predicted: '+ y_test + ' truth: ' + label2class[class_prediction[0]] )\n",
    "    plt.axis('tight')\n",
    "    image_num += 1\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(20, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6390ff4-a417-481f-b96f-02a73eb09e08",
   "metadata": {},
   "source": [
    "## Hopefully you have a unique image classifier !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd8dcdc-e049-454c-8e1d-4e926d8ef69d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
